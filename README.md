# Web Crawling 101 - On Going Project

Wondering what this is all about? Take 2 minutes to read our short [Open Code, Open Data Manifesto].

This project is structured to work as a series of classes focused on bootstrapping your data-mining / web-crawling knowledge. 
Some of the topics that are covered here:

 - [Anatomy of a Crawler (Policies and Behaviors)]
 - [Understanding HTTP Requests]
 - Scrapping / Parsing data out of HTML pages
 - Tooling (Frameworks and custom-made libraries)
 - Finding your public source of data
 - Modeling your objects
 - Storing your results
 - Scaling up your crawler

# How do I Start ?

Keep this project Wiki open at all times, since most of the text / references will be there for you to read, while you advance through the chapters/classes of this project.

Start each chapter by going to the Wiki first, and only after reading it's text, proceed to the code. 

Take your time, read the code comments, run it, modify it and run it again to understand the impact of each change.

Happy hacking :)

# Setup

1) Install pip (using terminal/command prompt navigate to the "Setup" directory and run `python get-pip.py`
2) Reload your terminal/command prompt (open and close)
3) Make sure pip is installed by running: `pip freeze`
4) If it is, you can now install the needed dependencies by running from the root of the project: 
`pip install -U -r Setup/requirements.txt`

# Version
0.0.5


[Anatomy of a Crawler (Policies and Behaviors)]:https://github.com/MarcelloLins/WebCrawling101/wiki/Chapter-1-:-Anatomy-of-a-Crawler

[Understanding HTTP Requests]:https://github.com/MarcelloLins/WebCrawling101/wiki/Chapter-2-:-Understanding-HTTP-Requests
[Open Code, Open Data Manifesto]:https://github.com/MarcelloLins/WebCrawling101/wiki/The-Open-Code-Open-Data-Manifesto
